---
pdf: http://proceedings.mlr.press/v29/Wang13b.pdf
title: Co-Training with Insufficient Views
abstract: Co-training is a famous semi-supervised learning paradigm exploiting unlabeled
  data with two views. Most previous theoretical analyses on co-training are based
  on the assumption that each of the views is sufficient to correctly predict the
  label. However, this assumption can hardly be met in real applications due to feature
  corruption or various feature noise. In this paper, we present the theoretical analysis
  on co-training when neither view is sufficient. We define the diversity between
  the two views with respect to the confidence of prediction and prove that if the
  two views have large diversity, co-training is able to improve the learning performance
  by exploiting unlabeled data even with insufficient views. We also discuss the relationship
  between view insufficiency and diversity, and give some implications for understanding
  of the difference between co-training and co-regularization.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Wang13b
month: 0
tex_title: Co-Training with Insufficient Views
firstpage: 467
lastpage: 482
page: 467-482
order: 467
cycles: false
author:
- given: Wei
  family: Wang
- given: Zhi-Hua
  family: Zhou
date: 2013-10-21
address: Australian National University, Canberra, Australia
publisher: PMLR
container-title: Proceedings of the 5th Asian Conference on Machine Learning
volume: '29'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 10
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
