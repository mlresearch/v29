---
pdf: http://proceedings.mlr.press/v29/Tran13.pdf
title: Improving Predictive Specificity of Description Logic Learners by Fortification
abstract: 'The predictive accuracy of a learning algorithm can be split into specificity
  and sensitivity, amongst other decompositions. Sensitivity, also known as completeness,
  is the ratio of true positives to the total number of positive examples, while specificity
  is the ratio of true negative to the total negative examples. In top-down learning
  methods of inductive logic programming, there is generally a bias towards sensitivity,
  since the learning starts from the most general rule (everything is positive) and
  specialises by excluding some of the negative examples. While this is often useful,
  it is not always the best choice: for example, in novelty detection, where the negative
  examples are rare and often varied, they may well be ignored by the learning. In
  this paper we introduce a method that attempts to remove the bias towards sensitivity
  by fortifying the model by computing and then including in the model some descriptions
  of the negative data even if they are considered redundant by the normal learning
  algorithm. We demonstrate the method on a set of standard datasets for description
  logic learning and show that the predictive accuracy increases.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Tran13
month: 0
firstpage: 419
lastpage: 434
page: 419-434
sections: 
author:
- given: An
  family: Tran
- given: Jens
  family: Dietrich
- given: Hans
  family: Guesgen
- given: Stephen
  family: Marsland
date: 2013-10-21
address: Australian National University, Canberra, Australia
publisher: PMLR
container-title: Proceedings of the 5th Asian Conference on Machine Learning
volume: '29'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 10
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
